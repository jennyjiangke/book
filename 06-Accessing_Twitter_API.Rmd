# Accessing Twitter's API in R

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Twitter is a unique source for data, for many reasons: along with text, it includes date and time information, and data on the type of device anr/or software used to post the tweet. What we often do *not* have reliable Twitter data on is location. There are two ways for Twitter to record your location: one is created during your Twitter account setup, where you (optionally) fill in where you are located. Many users fill in joke locations (The Milky Way, 3rd Rock From the Sun). To do something with location data, it'd have to be somewhat standardized, i.e. text inputs for Town and State.Without these prompts, values vary wildly, from 'North Dakota' to 'Brooklyn' to '27510,' so it's really not very useful data.

The other way Twitter will record your location is by geolocating your exact position. This has to be turned on or off, by choosing if Twitter can use your location on your device. Many users - most, in fact - have location services turned off. That means if you use geolocation data, you're only observing data from *users who have opted in to location services for twitter.* That's a subset of Twitter users that will drastically skew our results, so I also discourage its use.

One other caveat with Twitter, which can be frustrating to new users: you can't go back in time and look at every Tweet ever posted. (Access to this is referred to as Twitter's 'Firehose.') Just searching Twitter for a term will only return results from the last seven days. How can we get around this limitation?

One way is to analyze an individual account, or Twitter handle - this will allow us to download thousands of Tweets, assuming we're looking at a prolific user.

The other way, which is beyond the scope of this Chapter, is to set up a loop in R that grabs Tweets every minute, or hour, or day, and records them, over time, to a database. This (obviously) requires pre-planning, so you can't just do a lengthy report on the history and progression of the #MeToo movement on Twitter: you can only record the present and future. (Note that some interesting historical Twitter activity has been downloaded and shared online, such as the accounts of everyone in the Trump White House during his Presidency.)

Why is all of this so complicated? Well, Twitter's valuation is based almost entirely on this huge collection of data it's made (the Firehose), and granting access to historical Twitter data is one way Twitter actually makes money.

Like many proprietary online sources of data, Twitter gives data analysts this limited access to their real-time data via what's called an *API*, or Application Programming Interface. That means instead of downloading every tweet ever made onto your laptop, you can use this 'interface' to access the database of information online and only grab the relevant data. Another example may help clarify: How could we analyze the headlines of the New York Times over the last 50 years? We cannot download all of that text onto our computer, of course, but the Times provides limited historical access via their API as well. Again, the limitations are somewhat severe, as they are protecting their intellectual property: The NY Times API only allows you to get *ten results every minute.* In other words, you query the API, get 10 results, wait a minute, and then you can get the next 10 results.

Note: One of our media analytics students Tyler Oldham made a package that automates this process \[here\] ('https://github.com/toldham2/nytall').

## Making Your First Request to the New Twitter API v2

OK, so how do we gain access to an API? We'll use Twitter as our example, as it's overly complicated. But generally speaking, you need to have an account with the service. Twitter is free, and you need a Twitter account to access the API. (The New York Times is behind a paywall, but you can still create an account on their website and access the aPI for free without a subscription.)

So, first and foremost, create a Twitter account if you don't have one (don't worry, you never have to Tweet.)

Then, go to [dev.twitter.com](https://dev.twitter.com) while signed in. Now, we are going to apply the v2 Twitter API. It includes several new features and improvements compared to the previous version, Twitter API v1.1. One of the most significant changes in Twitter API v2 is the introduction of a new set of modular endpoints, which allow developers to access specific sets of data and functionality more easily.

You can follow this link (https://developer.twitter.com/en/docs/tutorials/step-by-step-guide-to-making-your-first-request-to-the-twitter-api-v2) to see Step-by-step guide to making your first request to the new Twitter API v2.

At this time, there are three different levels of access that are applied at the Project-level:

*Essential access*: This access level is provided to anyone who has signed up for a developer account. (free)
- Number of Apps within that Project: 1
- Tweet consumption cap: 500,000 Tweets per month

*Elevated access*: This access level is provided to anyone who has applied for additional access via the developer portal. (free)
- Number of Apps within that Project: 3
- Tweet consumption cap: 2 million Tweets per month

*Academic research access*: This access level is provided to academic researchers who meet a certain set of requirements and applied for additional access via the developer portal. (need to apply)
- Number of Apps within that Project: 1
- Tweet consumption cap: 10 million Tweets per month

## Using You Twitter App in R

Start by installing the 'rtweet' package from CRAN:

```{r, eval = FALSE}
install.packages('rtweet')
```

Once we've set up an App in the Twitter Developer Console, we need to connect our credentials to our R session.

First, we have to 'tell' Twitter who we are and what App we're using, buy entering in our API Key and Secret. Let's start by loading a package that will allow for authentication via a Web Browser:

```{r}

## load rtweet
library(rtweet)
```

Once our app is set up in dev.twitter.com, we should be able to generate *four secret codes*:

-   API Key (also known as *consumer key)*

-   API Secret (also known as *consumer secret)*

-   Access Token

-   Access Secret

    All of these values can be generated from the 'Keys and Tokens' tab of your App's page on dev.twitter.com.


    Run this in R - be sure to replace "YOUR_APP_NAME" and "XXXXX" with the name of your App in the Twitter Developer Portal:

```{r, eval = FALSE}

create_token(
app = "YOUR_APP_NAME_HERE",
consumer_key = "XXXXXXXXXXXXXXXXXXX",
consumer_secret = "XXXXXXXXXXXXXXXXXXX",
access_token = "XXXXXXXXXXXXXXXXXXX",
access_secret = "XXXXXXXXXXXXXXXXXXX",
set_renv = TRUE
) -> twitter_token
```

Let's grab the last 3,000+ tweets from the Pope:

```{r}

mytoken <- create_token(
  app = "MEA3290FALLTEACHING", #app name here
  consumer_key = "tKMrTovHc5WDej3skv50ydiBX", #consumer key here
  consumer_secret = "cuVLeICFGV82dyJaxL5wowmU6khsdIMPDZZJH6l0QpUaR16IKm", #consumer secret here
  access_token = "1942203541-j9vVz9wl3TinMD9JTzxW6SpkHt62zAE0lYune87", #access token here
  access_secret = "Cjaese8kXV3eVyGx9utFsX5khSzjYwzk1Jc0kpxvFBigs") #access secret here
```

```{r}
pope <- get_timeline("pontifex", n = 18000, retryonratelimit = TRUE, token = mytoken)
```

If you don't have any luck, you can download the version of the Pope's tweets that I'm using in my analysis here:

```{r}

library(downloadthis)

pope %>%
  download_this(
    output_name = "pope_tweets",
    output_extension = ".csv",
    button_label = "Download data",
    button_type = "default",
    self_contained = TRUE,
    has_icon = TRUE,
    icon = "fa fa-save",
    id = "pope-btn"
  )
```

You'll also need to import it - if you copy it into your Project's folder, you can simply run the command `pope <- read_csv('pope_tweets.csv')`

## Analyzing Twitter Data

OK, now we can use content from Twitter and analyze aspects such as:

-   Time, day and date of tweet frequency

-   Device(s) and App(s) used to Tweet

-   Lots of other metadata, like if each tweet was a retweet, or was flagged in certain counties, etc.

And of course we can collect *all of the text* from tweets and analyze them as we would a book.

Let's start with time.

### Lubridate & Working with Time

Lubridate is a really useful R package. You can ask it to break any collection of dates into days, weeks, months or years. And if it doesn't recognize the format of your date column, you can 'tell' it how to read correctly. Let's try using it on Twitter data.

```{r}
install.packages('lubridate')
```

The 'created_at' column of our Twitter data contains our date information. Let's try some things out:

```{r}
library(tidyverse)

ggplot(pope, aes(x = created_at)) + geom_histogram(aes(fill = ..count..))
```

OK, we see the frequency over the past few years that the Pope has tweeted - but it's pretty hard to understand. Let's tweak it with a lubridate command to show the days of the week:

```{r}
library(lubridate)
ggplot(pope, aes(x=wday(created_at, label = TRUE))) +
  geom_bar()

```

OK, that's much clearer and makes a lot of sense! 

### Tweet Source

Did the user tweet from their phone? From a computer? Did they use a social media app that help them maintain consistency across platforms? *Did an aide post it?*

These are the questions we can answer by analyzing the *source* column of Twitter data:

```{r}
ggplot(pope, aes(source)) + geom_bar(aes(fill = ..count..)) + coord_flip()
```

Wow, that's boring. Let's check if the President has a more varied source for tweets:

```{r}
prez <- get_timeline('potus', n=18000, retryonratelimit = TRUE)

ggplot(prez, aes(source)) + geom_bar(aes(fill = ..count..)) + coord_flip()

```
A few notes about our plots:

-   We are using geom_bar() instead of geom_col() - they look the same, but geom_col() often requires extra parameters like 'bin width.'

-   We earlier used geom_histogram. A *histogram* is like a line chart that 'bins' values into groups; think of plotting age: you wouldn't have 90 or 100 lines to represent every age in your survey, you'd *bin* them: 0-10 years, 11-20 years, etc. Histograms visualize binned, continuous data.

-   The syntax of our ggplot's is just *weird*. What's up with '`..count..`?' Why do we have two sets of aesthetics? Well, `..count..` is a *special variables* that can be used in ggplot - another is `..density..` . As for the separate aesthetics, the first set creates the bar chart based on the data values, and the second set colors the bars. We can control the aesthetics of those two calculations separately.

### Text Analysis in Twitter

We can again use the *tidytext* package to create and analyze tokens:

    ```{r}
       library(tidytext)
     pope %>% 
      unnest_tokens(word, text) %>% 
      anti_join(stop_words) %>% 
      count(word, sort = TRUE) 
    ```

That certainly looks correct, although there are a few words that don't seem to make sense: *t.co* and *https* . *t.co* is short for twitter.com, and the https is part of a link, of course. You may also see *amp*, which is short for *ampersand.*

Let's filter out these needless words:

```{r}
pope %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  filter(!word %in% c("https", "t.co")) %>% 
  count(word, sort = TRUE) 
```

Cool. How about a word cloud?

```{r}
library(wordcloud2)
pope %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  filter(!word %in% c("https", "t.co")) %>% 
  count(word, sort = TRUE) %>% 
  wordcloud2()
```
